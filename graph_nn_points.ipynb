{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dojian/MLproject/blob/main/graph_nn_points.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "gekXxxxH6KL1"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Created on Nov 26 2023\n",
        "\n",
        "@author: dojian\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import json\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pickle"
      ],
      "metadata": {
        "id": "yXt57ixs7XK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nUtUjI-U7QID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "X6sFEGyh6KL-"
      },
      "outputs": [],
      "source": [
        "class Data_Loader():\n",
        "\n",
        "    def __init__(self, data_file_path, label_file_path, n_images, chart_type):\n",
        "        self.data_file_path = data_file_path\n",
        "        self.label_file_path = label_file_path\n",
        "        self.n_images = n_images\n",
        "        self.chart_type = chart_type  # Added chart_type as an attribute\n",
        "\n",
        "    def load_image_data(self):\n",
        "        X = []\n",
        "        Y = []\n",
        "\n",
        "        for file in os.listdir(self.data_file_path):\n",
        "            file_path = os.path.join(self.data_file_path, file)\n",
        "\n",
        "            if file.endswith('.jpg'):\n",
        "                img_annotations = self.__load_annotations(file)\n",
        "\n",
        "                # Check if annotations exist and if chart-type matches self.chart_type\n",
        "                if img_annotations is not None and img_annotations.get('chart-type') == self.chart_type:\n",
        "                    Y.append(img_annotations)\n",
        "                    img = cv2.imread(file_path)\n",
        "                    X.append(img)\n",
        "\n",
        "                # Stop if enough images are loaded\n",
        "                if len(X) >= self.n_images:\n",
        "                    return X, Y\n",
        "\n",
        "        return X, Y\n",
        "\n",
        "    def __load_annotations(self, image_file_name):\n",
        "        file_name = image_file_name.split('.jpg')[0]\n",
        "        json_file_name = file_name + '.json'\n",
        "        json_file_path = os.path.join(self.label_file_path, json_file_name)\n",
        "\n",
        "        if os.path.isfile(json_file_path):\n",
        "            with open(json_file_path) as f:\n",
        "                return json.load(f)\n",
        "\n",
        "        else:\n",
        "            return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "hMfs0n866KMA"
      },
      "outputs": [],
      "source": [
        "class Image_Processor():\n",
        "\n",
        "    def __init__(self, images):\n",
        "        self.images = images\n",
        "        self.min_width, self.min_height = self.__find_smallest_image_width_and_height()\n",
        "        self.resized_images = self.__resize_images()\n",
        "\n",
        "\n",
        "    def __resize_images(self):\n",
        "\n",
        "        X_resized = []\n",
        "\n",
        "        for img in self.images:\n",
        "            X_resized.append(tf.image.resize(img,\n",
        "                                             size=(self.min_width, self.min_height)))\n",
        "\n",
        "        return np.array(X_resized)\n",
        "\n",
        "\n",
        "    def __find_smallest_image_width_and_height(self):\n",
        "\n",
        "        min_width = np.size(self.images[0], 0)\n",
        "        min_height = np.size(self.images[0], 1)\n",
        "\n",
        "        for img in self.images[1:]:\n",
        "\n",
        "            if np.size(img, 0) < min_width:\n",
        "                min_width = np.size(img, 0)\n",
        "\n",
        "            if np.size(img, 1) < min_height:\n",
        "                min_height = np.size(img, 1)\n",
        "\n",
        "        return min_width, min_height"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "vqmKOpgC6KMD",
        "outputId": "c144ec67-766d-4108-d17d-b14e768db09f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "import os\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mount your Google drive in Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2O0iDHV7tvh",
        "outputId": "b67a40e7-3ccc-4dac-f900-379cbb28f6cf"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load data from Kaggle\n",
        "!pip install -U -q kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!echo '{\"username\":\"angel80423\",\"key\":\"f5a347582d10b1f9f45bc7bb61ab390b\"}' > ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle competitions download -c benetech-making-graphs-accessible"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V891u-WV7vsf",
        "outputId": "662f640c-631e-443c-f5d0-204f884cf863"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "benetech-making-graphs-accessible.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q benetech-making-graphs-accessible.zip -d ./"
      ],
      "metadata": {
        "id": "qTEJfp6e9CDG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43014172-e276-4db6-867c-80f91b844d6b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "replace ./sample_submission.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "2vPJKEd36KMD"
      },
      "outputs": [],
      "source": [
        "data_file_path = '/content/train/images'\n",
        "label_file_path = '/content/train/annotations'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "thvcMYhl6KME"
      },
      "outputs": [],
      "source": [
        "n_images = 7000\n",
        "chart_type='scatter'\n",
        "\n",
        "# Load in raw data\n",
        "X_raw, Y_raw = Data_Loader(data_file_path,\n",
        "                           label_file_path, n_images,chart_type).load_image_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "O33GFZc86KMF"
      },
      "outputs": [],
      "source": [
        "# Process images: resize and rescale\n",
        "X_processed = Image_Processor(X_raw)\n",
        "X_resized = X_processed.resized_images\n",
        "X_scaled = X_resized / 255.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_scaled.shape)\n",
        "print(len(Y_raw))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rz2cfcuRgkxY",
        "outputId": "32ffea0a-0b6b-42c2-fba9-2d3d991eb380"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7000, 223, 318, 3)\n",
            "7000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Process labels: make scatters saved as an array of (x,y) points\n",
        "\n",
        "def process_labels(Y_raw):\n",
        "    Y_processed = []\n",
        "    for annotation in Y_raw:\n",
        "        # Extract (x,y) for each point from annotation\n",
        "        data_series = annotation['data-series']\n",
        "        points = [(point['x'], point['y']) for point in data_series]\n",
        "        Y_processed.append(points)\n",
        "\n",
        "    return np.array(Y_processed)\n",
        "\n",
        "Y_processed =process_labels(Y_raw)"
      ],
      "metadata": {
        "id": "N67imJrPSti_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df2ab0dc-4468-46fb-9f15-7d3c2ce8f73e"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-101-6886b0e1c1f2>:11: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  return np.array(Y_processed)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the maximum length\n",
        "max_length = max(len(item) for item in Y_processed)\n",
        "# Pad each sublist in Y_processed\n",
        "Y_padded = np.array([np.pad(item, ((0, max_length - len(item)), (0, 0)), mode='constant') for item in Y_processed])"
      ],
      "metadata": {
        "id": "aOnfuwb7p6jQ"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "2uSGnl4N6KMG"
      },
      "outputs": [],
      "source": [
        "# Shuffle and Split the Data\n",
        "shuffle_indices = tf.random.shuffle(tf.range(tf.shape(X_scaled)[0], dtype=tf.int32))\n",
        "X_shuffled = tf.gather(X_scaled, shuffle_indices)\n",
        "Y_shuffled = tf.gather(Y_padded, shuffle_indices)\n",
        "\n",
        "split_index = int(0.8 * len(X_shuffled))\n",
        "\n",
        "X_train, X_val = X_shuffled[:split_index], X_shuffled[split_index:]\n",
        "Y_train, Y_val = Y_shuffled[:split_index], Y_shuffled[split_index:]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_val shape:\", X_val.shape)\n",
        "print(\"Y_train shape:\", Y_train.shape)\n",
        "print(\"Y_val shape:\", Y_val.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EATHF5gwP5Cs",
        "outputId": "431866ac-7505-4adc-b79f-7f41832a369b"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (5600, 223, 318, 3)\n",
            "X_val shape: (1400, 223, 318, 3)\n",
            "Y_train shape: (5600, 100, 2)\n",
            "Y_val shape: (1400, 100, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(input_shape):\n",
        "\n",
        "    model = tf.keras.Sequential()\n",
        "\n",
        "    # add first convolution layer to the model\n",
        "    model.add(tf.keras.layers.Conv2D(\n",
        "        filters=32,\n",
        "        kernel_size=(5, 5),\n",
        "        strides=(1, 1),\n",
        "        padding='same',\n",
        "        data_format='channels_last',\n",
        "        name='conv_1',\n",
        "        activation='relu'))\n",
        "\n",
        "\n",
        "    # add a max pooling layer with pool size (2,2) and strides of 2\n",
        "    # (this will reduce the spatial dimensions by half)\n",
        "    model.add(tf.keras.layers.MaxPool2D(\n",
        "        pool_size=(2, 2),\n",
        "        name='pool_1'))\n",
        "\n",
        "\n",
        "    # add second convolutional layer\n",
        "    # model.add(tf.keras.layers.Conv2D(\n",
        "    #     filters=64,\n",
        "    #     kernel_size=(5, 5),\n",
        "    #     strides=(1, 1),\n",
        "    #     padding='same',\n",
        "    #     name='conv_2',\n",
        "    #     activation='relu'))\n",
        "\n",
        "    # # add second max pooling layer with pool size (2,2) and strides of 2\n",
        "    # # (this will further reduce the spatial dimensions by half)\n",
        "    # model.add(tf.keras.layers.MaxPool2D(\n",
        "    #     pool_size=(2, 2), name='pool_2')\n",
        "    # )\n",
        "\n",
        "\n",
        "    ## add a fully connected layer (need to flatten the output of the previous layers first)\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    #model.add(tf.keras.layers.Dense(\n",
        "    #    units=16,\n",
        "     #   name='fc_1',\n",
        "     #   activation='relu'))\n",
        "\n",
        "    # add dropout layer\n",
        "    #model.add(tf.keras.layers.Dropout(\n",
        "    #     rate=0.5))\n",
        "\n",
        "    # add the last fully connected layer\n",
        "    # this last layer sets the activation function to \"None\" in order to output the logits\n",
        "    model.add(tf.keras.layers.Dense(\n",
        "        units=max_length*2,\n",
        "        name='fc_1',\n",
        "        activation=None))\n",
        "\n",
        "    # Reshape the output to (batch_size, 70, 2)\n",
        "    model.add(tf.keras.layers.Reshape((100, 2)))\n",
        "\n",
        "    tf.random.set_seed(1)\n",
        "    model.build(input_shape=input_shape)\n",
        "    model.summary()\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "aqMEgUB8zpV4"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(\n",
        "     input_shape=(None, X_processed.min_width, X_processed.min_height, 3))\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "               loss=tf.keras.losses.MeanSquaredError(),\n",
        "               metrics=['mae'])\n",
        "\n",
        "tf.random.set_seed(1234)\n",
        "np.random.seed(1234)\n",
        "history = model.fit(X_train, Y_train,\n",
        "                     epochs=20,\n",
        "                     batch_size=32,\n",
        "                     validation_data=(X_val, Y_val) )"
      ],
      "metadata": {
        "id": "4upw2hD3zULt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb8c2ef3-e171-474d-9e66-07561b3d64a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_21\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv_1 (Conv2D)             (None, 223, 318, 32)      2432      \n",
            "                                                                 \n",
            " pool_1 (MaxPooling2D)       (None, 111, 159, 32)      0         \n",
            "                                                                 \n",
            " flatten_20 (Flatten)        (None, 564768)            0         \n",
            "                                                                 \n",
            " fc_1 (Dense)                (None, 200)               112953800 \n",
            "                                                                 \n",
            " reshape_5 (Reshape)         (None, 100, 2)            0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 112956232 (430.89 MB)\n",
            "Trainable params: 112956232 (430.89 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "145/175 [=======================>......] - ETA: 38s - loss: 1948138512318464.0000 - mae: 755447.5000"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "psZy1w5d7OqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "ks2KIImDKfEB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}