{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gekXxxxH6KL1"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import json\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "X6sFEGyh6KL-"
      },
      "outputs": [],
      "source": [
        "class Data_Loader():\n",
        "\n",
        "    def __init__(self, data_file_path, label_file_path, n_images, chart_type):\n",
        "        self.data_file_path = data_file_path\n",
        "        self.label_file_path = label_file_path\n",
        "        self.n_images = n_images\n",
        "        self.chart_type = chart_type  # Added chart_type as an attribute\n",
        "\n",
        "    def load_image_data(self):\n",
        "        X = []\n",
        "        Y = []\n",
        "\n",
        "        for file in os.listdir(self.data_file_path):\n",
        "            file_path = os.path.join(self.data_file_path, file)\n",
        "\n",
        "            if file.endswith('.jpg'):\n",
        "                img_annotations = self.__load_annotations(file)\n",
        "\n",
        "                # Check if annotations exist and if chart-type matches self.chart_type\n",
        "                if img_annotations is not None and img_annotations.get('chart-type') == self.chart_type:\n",
        "                    Y.append(img_annotations)\n",
        "                    img = cv2.imread(file_path)\n",
        "                    X.append(img)\n",
        "\n",
        "                # Stop if enough images are loaded\n",
        "                if len(X) >= self.n_images:\n",
        "                    return X, Y\n",
        "\n",
        "        return X, Y\n",
        "\n",
        "    def __load_annotations(self, image_file_name):\n",
        "        file_name = image_file_name.split('.jpg')[0]\n",
        "        json_file_name = file_name + '.json'\n",
        "        json_file_path = os.path.join(self.label_file_path, json_file_name)\n",
        "\n",
        "        if os.path.isfile(json_file_path):\n",
        "            with open(json_file_path) as f:\n",
        "                return json.load(f)\n",
        "\n",
        "        else:\n",
        "            return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hMfs0n866KMA"
      },
      "outputs": [],
      "source": [
        "class Image_Processor():\n",
        "\n",
        "    def __init__(self, images):\n",
        "        self.images = images\n",
        "        self.min_width, self.min_height = self.__find_smallest_image_width_and_height()\n",
        "        self.resized_images = self.__resize_images()\n",
        "\n",
        "\n",
        "    def __resize_images(self):\n",
        "\n",
        "        X_resized = []\n",
        "\n",
        "        for img in self.images:\n",
        "            X_resized.append(tf.image.resize(img,\n",
        "                                             size=(self.min_width, self.min_height)))\n",
        "\n",
        "        return np.array(X_resized)\n",
        "\n",
        "\n",
        "    def __find_smallest_image_width_and_height(self):\n",
        "\n",
        "        min_width = np.size(self.images[0], 0)\n",
        "        min_height = np.size(self.images[0], 1)\n",
        "\n",
        "        for img in self.images[1:]:\n",
        "\n",
        "            if np.size(img, 0) < min_width:\n",
        "                min_width = np.size(img, 0)\n",
        "\n",
        "            if np.size(img, 1) < min_height:\n",
        "                min_height = np.size(img, 1)\n",
        "\n",
        "        return min_width, min_height"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "vqmKOpgC6KMD",
        "outputId": "2bf881cf-c1cf-4aa5-f32d-6cb412bca439"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import os\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mount your Google drive in Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2O0iDHV7tvh",
        "outputId": "2dff5415-233c-4185-9b7d-2b657b00d57a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load data from Kaggle\n",
        "!pip install -U -q kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!echo '{\"username\":\"angel80423\",\"key\":\"f5a347582d10b1f9f45bc7bb61ab390b\"}' > ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle competitions download -c benetech-making-graphs-accessible"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V891u-WV7vsf",
        "outputId": "000620c0-9a7f-433b-eeef-d01075a94c4d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading benetech-making-graphs-accessible.zip to /content\n",
            "100% 982M/982M [00:38<00:00, 28.3MB/s]\n",
            "100% 982M/982M [00:38<00:00, 26.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q benetech-making-graphs-accessible.zip -d ./"
      ],
      "metadata": {
        "id": "qTEJfp6e9CDG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2vPJKEd36KMD"
      },
      "outputs": [],
      "source": [
        "data_file_path = '/content/train/images'\n",
        "label_file_path = '/content/train/annotations'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "thvcMYhl6KME"
      },
      "outputs": [],
      "source": [
        "n_images = 700\n",
        "chart_type='scatter'\n",
        "\n",
        "# Load in raw data\n",
        "X_raw, Y_raw = Data_Loader(data_file_path,\n",
        "                           label_file_path, n_images,chart_type).load_image_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "O33GFZc86KMF"
      },
      "outputs": [],
      "source": [
        "# Process images: resize and make it gray\n",
        "X_processed = Image_Processor(X_raw)\n",
        "X_resized = X_processed.resized_images\n",
        "X_gray = X_resized / 255.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Process labels: make scatters saved as an array of (id,x,y) points\n",
        "\n",
        "def process_labels(Y_raw):\n",
        "    Y_processed = []\n",
        "    for annotation in Y_raw:\n",
        "        # Extract polygonï¼ˆx,y) for each point from annotation\n",
        "        text_annotations = annotation['text']\n",
        "        for text_annotation in text_annotations:\n",
        "            polygon = text_annotation['polygon']\n",
        "            id = text_annotation.get('id', -1)  # Default to -1 if no id is present\n",
        "            # Loop through each pair of coordinates\n",
        "            num_points = len(polygon) // 2\n",
        "            for i in range(num_points):\n",
        "                        x_key = 'x' + str(i)\n",
        "                        y_key = 'y' + str(i)\n",
        "                        if x_key in polygon and y_key in polygon:\n",
        "                            point = [id, polygon[x_key], polygon[y_key]]\n",
        "                            Y_processed.append(point)\n",
        "\n",
        "    return np.array(Y_processed)\n",
        "\n",
        "Y_processed = process_labels(Y_raw)"
      ],
      "metadata": {
        "id": "N67imJrPSti_"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "2uSGnl4N6KMG"
      },
      "outputs": [],
      "source": [
        "# Shuffle and Split the Data\n",
        "shuffle_indices = tf.random.shuffle(tf.range(tf.shape(X_gray)[0], dtype=tf.int32))\n",
        "X_shuffled = tf.gather(X_gray, shuffle_indices)\n",
        "Y_shuffled = tf.gather(Y_processed, shuffle_indices)\n",
        "\n",
        "split_index = int(0.8 * len(X_shuffled))\n",
        "\n",
        "X_train, X_val = X_shuffled[:split_index], X_shuffled[split_index:]\n",
        "Y_train, Y_val = Y_shuffled[:split_index], Y_shuffled[split_index:]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_val shape:\", X_val.shape)\n",
        "print(\"Y_train shape:\", Y_train.shape)\n",
        "print(\"Y_val shape:\", Y_val.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EATHF5gwP5Cs",
        "outputId": "02ce0b53-0ce8-49b9-95cc-b2078b81b457"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (560, 248, 447, 3)\n",
            "X_val shape: (140, 248, 447, 3)\n",
            "Y_train shape: (560, 3)\n",
            "Y_val shape: (140, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(input_shape):\n",
        "\n",
        "    model = tf.keras.Sequential()\n",
        "\n",
        "    # add first convolution layer to the model\n",
        "    model.add(tf.keras.layers.Conv2D(\n",
        "        filters=16,\n",
        "        kernel_size=(3, 3),\n",
        "        strides=(1, 1),\n",
        "        padding='same',\n",
        "        data_format='channels_last',\n",
        "        name='conv_1',\n",
        "        activation='relu'))\n",
        "\n",
        "\n",
        "    # add a max pooling layer with pool size (2,2) and strides of 2\n",
        "    # (this will reduce the spatial dimensions by half)\n",
        "    model.add(tf.keras.layers.MaxPool2D(\n",
        "        pool_size=(2, 2),\n",
        "        name='pool_1'))\n",
        "\n",
        "\n",
        "    # add second convolutional layer\n",
        "    # model.add(tf.keras.layers.Conv2D(\n",
        "    #     filters=64,\n",
        "    #     kernel_size=(5, 5),\n",
        "    #     strides=(1, 1),\n",
        "    #     padding='same',\n",
        "    #     name='conv_2',\n",
        "    #     activation='relu'))\n",
        "\n",
        "    # # add second max pooling layer with pool size (2,2) and strides of 2\n",
        "    # # (this will further reduce the spatial dimensions by half)\n",
        "    # model.add(tf.keras.layers.MaxPool2D(\n",
        "    #     pool_size=(2, 2), name='pool_2')\n",
        "    # )\n",
        "\n",
        "\n",
        "    # add a fully connected layer (need to flatten the output of the previous layers first)\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    model.add(tf.keras.layers.Dense(\n",
        "        units=128,\n",
        "        name='fc_1',\n",
        "        activation='relu'))\n",
        "\n",
        "    # add dropout layer\n",
        "    #model.add(tf.keras.layers.Dropout(\n",
        "    #     rate=0.5))\n",
        "\n",
        "    # add the last fully connected layer\n",
        "    # this last layer sets the activation function to \"None\" in order to output the logits\n",
        "    # note that passing activation = \"sigmoid\" will return class memembership probabilities but\n",
        "    # in TensorFlow logits are prefered for numerical stability\n",
        "    model.add(tf.keras.layers.Dense(\n",
        "        units=1,\n",
        "        name='fc_2',\n",
        "        activation=None))\n",
        "\n",
        "    tf.random.set_seed(1)\n",
        "    model.build(input_shape=input_shape)\n",
        "    model.summary()\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "aqMEgUB8zpV4"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(\n",
        "     input_shape=(None, X_processed.min_width, X_processed.min_height, 3))\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "               loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "               metrics=['accuracy'])\n",
        "\n",
        "tf.random.set_seed(1234)\n",
        "np.random.seed(1234)\n",
        "history = model.fit(X_train, Y_train,\n",
        "                     epochs=10,\n",
        "                     batch_size=96,\n",
        "                     validation_data=(X_val, Y_val) )"
      ],
      "metadata": {
        "id": "4upw2hD3zULt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d1641a6c-1663-48c8-b3e0-ee453401e76b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv_1 (Conv2D)             (None, 248, 447, 16)      448       \n",
            "                                                                 \n",
            " pool_1 (MaxPooling2D)       (None, 124, 223, 16)      0         \n",
            "                                                                 \n",
            " flatten_4 (Flatten)         (None, 442432)            0         \n",
            "                                                                 \n",
            " fc_1 (Dense)                (None, 128)               56631424  \n",
            "                                                                 \n",
            " fc_2 (Dense)                (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 56632001 (216.03 MB)\n",
            "Trainable params: 56632001 (216.03 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-c5acaeec51e6>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1234\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1234\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m history = model.fit(X_train, Y_train,\n\u001b[0m\u001b[1;32m     11\u001b[0m                      \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                      \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m96\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     ]\n\u001b[0;32m---> 60\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     61\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits defined at (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n\n  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"<ipython-input-20-c5acaeec51e6>\", line 10, in <cell line: 10>\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1783, in fit\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1377, in train_function\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1360, in step_function\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1349, in run_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1127, in train_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1185, in compute_loss\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 143, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 270, in call\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 2454, in sparse_categorical_crossentropy\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend.py\", line 5777, in sparse_categorical_crossentropy\n\nlogits and labels must have the same first dimension, got logits shape [96,1] and labels shape [288]\n\t [[{{node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_train_function_8303]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ks2KIImDKfEB",
        "outputId": "59bff96a-abb2-467b-9829-af28a2b57adc"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}